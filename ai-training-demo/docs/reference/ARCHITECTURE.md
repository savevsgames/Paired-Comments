# AI Training Demo - System Architecture

**Version:** 0.1.0
**Last Updated:** October 19, 2025

---

## ğŸ—ï¸ System Overview

The AI Training Comparison Demo is a full-stack system for proving that `.comments` metadata improves AI model training outcomes. It consists of four major components:

1. **Data Pipeline** - Export `.comments` to training data (JSONL)
2. **Training System** - Azure ML fine-tuning pipelines
3. **Evaluation Harness** - Automated metrics calculation
4. **Dashboard** - Public-facing comparison visualization

---

## ğŸ“ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AI Training Demo System                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Repos    â”‚  (React, Express, Python projects)
â”‚  with .comments  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA PIPELINE (Phase 1)                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. MCP Server Export Tool                                           â”‚
â”‚     - export_comments(format: "jsonl", includeSourceContext: true)   â”‚
â”‚     - Outputs JSONL with code context (Â±5 lines)                     â”‚
â”‚                                                                       â”‚
â”‚  2. Dataset Splitter                                                 â”‚
â”‚     - Split into train/validation/test (70/15/15)                    â”‚
â”‚     - Create control dataset (code only)                             â”‚
â”‚     - Create experiment dataset (code + .comments metadata)          â”‚
â”‚                                                                       â”‚
â”‚  3. Quality Validator                                                â”‚
â”‚     - Check for syntax errors (ESLint, Pylint)                       â”‚
â”‚     - Verify metadata completeness                                   â”‚
â”‚     - Manual review queue (human-in-the-loop)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AZURE BLOB STORAGE                                                  â”‚
â”‚  - datasets/control/train.jsonl         (350 samples)                â”‚
â”‚  - datasets/control/validation.jsonl    (75 samples)                 â”‚
â”‚  - datasets/control/test.jsonl          (75 samples)                 â”‚
â”‚  - datasets/experiment/train.jsonl      (350 samples)                â”‚
â”‚  - datasets/experiment/validation.jsonl (75 samples)                 â”‚
â”‚  - datasets/experiment/test.jsonl       (75 samples)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRAINING SYSTEM (Phase 2)                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Azure Machine Learning Studio                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Control Pipeline  â”‚        â”‚ Experiment Pipelineâ”‚               â”‚
â”‚  â”‚  (Conventional)    â”‚        â”‚ (+ .comments)      â”‚               â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚  â”‚ 1. Load train.jsonlâ”‚        â”‚ 1. Load train.jsonlâ”‚               â”‚
â”‚  â”‚ 2. Fine-tune GPT-4 â”‚        â”‚ 2. Fine-tune GPT-4 â”‚               â”‚
â”‚  â”‚    or Llama-3      â”‚        â”‚    or Llama-3      â”‚               â”‚
â”‚  â”‚ 3. Track metrics   â”‚        â”‚ 3. Track metrics   â”‚               â”‚
â”‚  â”‚    (MLflow)        â”‚        â”‚    (MLflow)        â”‚               â”‚
â”‚  â”‚ 4. Save checkpoint â”‚        â”‚ 4. Save checkpoint â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â”‚                             â”‚                            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                      â–¼                                               â”‚
â”‚              Model Artifacts                                         â”‚
â”‚              - control-model.bin                                     â”‚
â”‚              - experiment-model.bin                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EVALUATION HARNESS (Phase 3)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  For each model (control + experiment):                              â”‚
â”‚                                                                       â”‚
â”‚  1. Code Accuracy Evaluator                                          â”‚
â”‚     - Generate code for 75 test prompts                              â”‚
â”‚     - Compare AST against expected output                            â”‚
â”‚     - Calculate exact match %                                        â”‚
â”‚                                                                       â”‚
â”‚  2. Functional Correctness Evaluator                                 â”‚
â”‚     - Execute generated code in sandbox                              â”‚
â”‚     - Run unit tests                                                 â”‚
â”‚     - Calculate pass rate %                                          â”‚
â”‚                                                                       â”‚
â”‚  3. Semantic Similarity Evaluator                                    â”‚
â”‚     - Embed generated code (OpenAI embeddings)                       â”‚
â”‚     - Calculate cosine similarity to target                          â”‚
â”‚     - Calculate average similarity score                             â”‚
â”‚                                                                       â”‚
â”‚  4. Pass@k Evaluator                                                 â”‚
â”‚     - Generate k=10 samples per prompt                               â”‚
â”‚     - Calculate % with at least 1 correct answer                     â”‚
â”‚                                                                       â”‚
â”‚  5. Human Eval Interface                                             â”‚
â”‚     - Side-by-side A/B comparison UI                                 â”‚
â”‚     - Collect preferences from 10+ reviewers                         â”‚
â”‚     - Calculate preference %                                         â”‚
â”‚                                                                       â”‚
â”‚  6. Statistical Analysis                                             â”‚
â”‚     - Run t-test for significance (p < 0.05)                         â”‚
â”‚     - Calculate effect size (Cohen's d)                              â”‚
â”‚     - Generate comparison tables                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESULTS DATABASE (JSON)                                             â”‚
â”‚  {                                                                    â”‚
â”‚    "scenario": "react-components",                                   â”‚
â”‚    "control": { "accuracy": 0.78, "functional": 0.72, ... },         â”‚
â”‚    "experiment": { "accuracy": 0.94, "functional": 0.89, ... },      â”‚
â”‚    "improvement": { "accuracy": "+16%", "functional": "+17%", ... }, â”‚
â”‚    "significance": { "p_value": 0.003, "effect_size": 1.2 }          â”‚
â”‚  }                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DASHBOARD (Phase 4) - Next.js App                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Split-Screen Training Visualization                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚ â”‚
â”‚  â”‚  â”‚  Control Model       â”‚  â”‚  Experiment Model    â”‚           â”‚ â”‚
â”‚  â”‚  â”‚  (Conventional)      â”‚  â”‚  (+ .comments)       â”‚           â”‚ â”‚
â”‚  â”‚  â”‚                      â”‚  â”‚                      â”‚           â”‚ â”‚
â”‚  â”‚  â”‚  Training Loss: 0.45 â”‚  â”‚  Training Loss: 0.32 â”‚           â”‚ â”‚
â”‚  â”‚  â”‚  Accuracy: 78%       â”‚  â”‚  Accuracy: 94%       â”‚  (+16%)   â”‚ â”‚
â”‚  â”‚  â”‚  Pass@k: 65%         â”‚  â”‚  Pass@k: 83%         â”‚  (+18%)   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Model Comparison UI                                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚ â”‚
â”‚  â”‚  â”‚  Control Output      â”‚  â”‚  Experiment Output   â”‚           â”‚ â”‚
â”‚  â”‚  â”‚  (Side-by-side diff) â”‚  â”‚  (Annotated)         â”‚           â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ â”‚
â”‚  â”‚  Annotations: "Metadata helped with type inference here â†‘"     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  "Try It Yourself" Interactive Prompt                          â”‚ â”‚
â”‚  â”‚  Prompt: "Create a login form component with validation"       â”‚ â”‚
â”‚  â”‚  [Generate with Both Models]                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Export Charts & Tables                                        â”‚ â”‚
â”‚  â”‚  [Download PNG] [Download CSV] [Copy LaTeX Table]              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              Public Demo URL
       (ai-training-demo.vercel.app)
```

---

## ğŸ”§ Component Details

### 1. Data Pipeline

**Input:** Source repositories with `.comments` files
**Output:** JSONL training datasets

**Tech Stack:**
- MCP Server (existing) for `.comments` export
- Python scripts for dataset splitting
- ESLint/Pylint for quality validation

**Data Format (JSONL):**
```json
{
  "prompt": "Create a React form input component with accessibility",
  "completion": "const FormInput = ({ label, ...props }) => {\n  return (\n    <div>\n      <label htmlFor={props.id}>{label}</label>\n      <input {...props} />\n    </div>\n  );\n};",
  "metadata": {
    "componentType": "FormInput",
    "propCount": 5,
    "a11y": true,
    "responsive": true,
    "complexity": 3
  },
  "sourceContext": {
    "beforeLines": ["// Previous function...", ""],
    "afterLines": ["", "// Next function..."]
  }
}
```

**Control Dataset:** Same prompt/completion, but `metadata` field is EMPTY
**Experiment Dataset:** Full metadata from `.comments` (params + aiMeta)

---

### 2. Training System

**Platform:** Azure Machine Learning Studio
**Alternative:** Hugging Face AutoTrain (fallback)

**Models:**
- **Option A:** GPT-4 fine-tuning API (expensive, high quality)
- **Option B:** Llama-3 70B (cheaper, open source)
- **Option C:** Claude fine-tuning (if API available)

**Hyperparameters:**
```python
{
  "learning_rate": 1e-5,
  "batch_size": 4,
  "epochs": 3,
  "warmup_steps": 100,
  "max_seq_length": 2048,
  "early_stopping": True,
  "patience": 2
}
```

**Training Pipeline (Azure ML):**
```python
from azure.ai.ml import MLClient, command
from azure.ai.ml.entities import Environment

# 1. Create compute cluster
compute = ml_client.compute.begin_create_or_update(
    AmlCompute(name="gpu-cluster", size="Standard_NC6s_v3", max_instances=2)
).result()

# 2. Define training job
job = command(
    code="./src/training",
    command="python finetune.py --dataset ${{inputs.dataset}} --model ${{inputs.model}}",
    inputs={
        "dataset": Input(path="azureml://datasets/control-train"),
        "model": "meta-llama/Llama-3-70b"
    },
    environment="azureml://environments/pytorch-gpu",
    compute="gpu-cluster",
    experiment_name="paired-comments-control"
)

# 3. Submit job
ml_client.jobs.create_or_update(job)

# 4. Track with MLflow
import mlflow
mlflow.log_metrics({"train_loss": 0.45, "val_accuracy": 0.78})
```

---

### 3. Evaluation Harness

**Architecture:**
```
evaluation/
â”œâ”€â”€ benchmarks/          # Test suites for each scenario
â”‚   â”œâ”€â”€ react-components.test.ts
â”‚   â”œâ”€â”€ express-api.test.ts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ evaluators/          # Metric calculators
â”‚   â”œâ”€â”€ CodeAccuracy.ts
â”‚   â”œâ”€â”€ FunctionalCorrectness.ts
â”‚   â”œâ”€â”€ SemanticSimilarity.ts
â”‚   â””â”€â”€ PassAtK.ts
â”œâ”€â”€ harness.ts           # Main evaluation runner
â””â”€â”€ results/             # Output JSON files
```

**Evaluation Flow:**
```typescript
// harness.ts
async function evaluateModel(model: Model, testSet: TestCase[]) {
  const results = {
    accuracy: 0,
    functional: 0,
    semantic: 0,
    passAtK: 0
  };

  for (const testCase of testSet) {
    // Generate code
    const generated = await model.generate(testCase.prompt);

    // Run evaluators
    results.accuracy += CodeAccuracy.evaluate(generated, testCase.expected);
    results.functional += FunctionalCorrectness.execute(generated, testCase.tests);
    results.semantic += SemanticSimilarity.compare(generated, testCase.expected);
    results.passAtK += PassAtK.check(model, testCase.prompt, k=10);
  }

  return results;
}
```

**Statistical Analysis:**
```typescript
import { ttest } from 'simple-statistics';

const controlScores = [0.78, 0.72, 0.81, ...];
const experimentScores = [0.94, 0.89, 0.97, ...];

const pValue = ttest(controlScores, experimentScores);
const effectSize = cohensD(controlScores, experimentScores);

console.log(`p-value: ${pValue}`);  // Should be < 0.05
console.log(`Effect size: ${effectSize}`);  // Cohen's d > 0.8 = large effect
```

---

### 4. Dashboard (Next.js)

**Tech Stack:**
- **Framework:** Next.js 14 (App Router)
- **UI:** Tailwind CSS + shadcn/ui
- **Charts:** Recharts or Chart.js
- **Deployment:** Vercel or Azure Static Web Apps

**Project Structure:**
```
src/dashboard/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx                    # Home page (landing)
â”‚   â”œâ”€â”€ training/page.tsx           # Split-screen training viz
â”‚   â”œâ”€â”€ comparison/page.tsx         # Model comparison UI
â”‚   â”œâ”€â”€ playground/page.tsx         # "Try it yourself"
â”‚   â””â”€â”€ results/page.tsx            # Metrics tables
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ SplitScreenChart.tsx        # Real-time training metrics
â”‚   â”œâ”€â”€ CodeDiff.tsx                # Side-by-side comparison
â”‚   â”œâ”€â”€ MetricsTable.tsx            # Results table
â”‚   â””â”€â”€ PromptInterface.tsx         # Interactive testing
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ api.ts                      # Fetch results from JSON
â”‚   â”œâ”€â”€ models.ts                   # Load model APIs
â”‚   â””â”€â”€ types.ts                    # TypeScript types
â””â”€â”€ public/
    â””â”€â”€ results/                    # Static result JSON files
```

**Key Features:**
- **Live Training Viz:** Real-time charts showing loss/accuracy curves
- **Annotated Diffs:** Highlight where metadata helped (e.g., "Type inference from params here")
- **Interactive Prompts:** Users can test both models with custom prompts
- **Export:** Download charts as PNG, tables as CSV/LaTeX

---

## ğŸ”„ Data Flow

**Training Phase:**
```
.comments files
  â†’ MCP Server export (JSONL)
  â†’ Dataset splitter (train/val/test)
  â†’ Quality validation
  â†’ Azure Blob Storage
  â†’ Azure ML fine-tuning
  â†’ Trained model checkpoints
```

**Evaluation Phase:**
```
Trained models
  â†’ Load test set (75 prompts)
  â†’ Generate code for each prompt
  â†’ Run evaluators (accuracy, functional, semantic, pass@k)
  â†’ Calculate metrics
  â†’ Statistical analysis (t-test, effect size)
  â†’ Save results JSON
```

**Dashboard Phase:**
```
Results JSON
  â†’ Next.js API routes
  â†’ React components (charts, tables, diffs)
  â†’ Public demo URL
  â†’ User interaction (try prompts, export charts)
```

---

## ğŸ›¡ï¸ Security & Privacy

**Dataset Privacy:**
- Only use public repos (MIT/Apache licensed)
- Strip PII (names, emails, API keys) from code samples
- Anonymize author names in `.comments`

**Model Access Control:**
- Trained models are PRIVATE (not published)
- Only expose via demo UI (no direct API access)
- Rate limiting on "Try it yourself" (100 requests/day)

**Azure Security:**
- Use managed identities (no API keys in code)
- Enable Azure Key Vault for secrets
- Restrict network access to compute clusters

---

## ğŸ“Š Performance Targets

**Training Time:**
- GPT-4 fine-tuning: ~2-4 hours per run
- Llama-3 70B: ~6-12 hours per run (on 4x A100 GPUs)

**Evaluation Time:**
- 75 test cases Ã— 2 models = 150 evaluations
- ~30 seconds per evaluation (including execution)
- Total: ~75 minutes per scenario

**Dashboard Load Time:**
- Initial page load: < 2 seconds
- Chart rendering: < 500ms
- Interactive prompts: < 5 seconds per generation

---

## ğŸ”® Future Enhancements

**Phase 6+ (Post-Launch):**
- [ ] Real-time training updates (WebSocket streaming)
- [ ] User-contributed scenarios (community datasets)
- [ ] Multi-model comparison (GPT-4 vs Claude vs Llama)
- [ ] Automated hyperparameter tuning (Azure ML AutoML)
- [ ] A/B testing framework for new metadata types
- [ ] Export to academic paper format (IEEE, ACM)

---

## ğŸ“š References

- [Azure Machine Learning Docs](https://learn.microsoft.com/en-us/azure/machine-learning/)
- [GPT-4 Fine-Tuning API](https://platform.openai.com/docs/guides/fine-tuning)
- [HumanEval Benchmark (Pass@k)](https://github.com/openai/human-eval)
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [Next.js Documentation](https://nextjs.org/docs)

---

**Last Updated:** October 19, 2025
**Status:** Ready for Implementation
